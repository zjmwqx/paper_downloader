{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, re, os, errno, requests, urllib\n",
    "from BeautifulSoup import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASEURL = \"http://papers.nips.cc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linkre = re.compile('([0-9]+)\\.pdf')\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST:\n",
    "            pass\n",
    "        else: \n",
    "            raise\n",
    "\n",
    "\n",
    "def lencheck(element, length=1):\n",
    "    if len(element) < length:\n",
    "        raise RuntimeError, \"parse error %s\" % (str(element))\n",
    "\n",
    "\n",
    "def get_year_page(year):\n",
    "    bpreq = requests.get(BASEURL)\n",
    "    if not bpreq:\n",
    "        raise RuntimeError, \"could not download %s\" % (BASEURL)\n",
    "    soup = BeautifulSoup(bpreq.text)\n",
    "    years = soup.findAll(u\"a\", \n",
    "                attrs={u\"href\": re.compile(r'.book.*?%i.*' % (year))})\n",
    "    if len(years) < 1:\n",
    "        raise RuntimeError, \"year %i not found\" % (year)\n",
    "    yearurl = BASEURL + years[0][u\"href\"]\n",
    "    yearpage = requests.get(yearurl)\n",
    "    if not yearpage:\n",
    "        raise RuntimeError, \"could not download %s\" % (yearurl)\n",
    "    return yearpage.text\n",
    "\n",
    "\n",
    "def get_all_papers_on_yearpage(yearpage):\n",
    "    soup = BeautifulSoup(yearpage)\n",
    "    links = soup.findAll(u\"a\")\n",
    "    paperre = re.compile(r'(.*?paper.([0-9]+).*)')\n",
    "    results = []\n",
    "    for l in links:\n",
    "        ret = paperre.match(l[u\"href\"])\n",
    "        if ret:\n",
    "            results.append(ret.groups())\n",
    "    return results\n",
    "\n",
    "\n",
    "def strip_slashes(x):\n",
    "    return re.sub(r'/', '', x)\n",
    "\n",
    "\n",
    "def download_pdf_from_paperpage(url):\n",
    "    paperpage = requests.get(BASEURL + url)\n",
    "    if not paperpage:\n",
    "        raise RuntimeError, \"could not download %s\" % (BASEURL + url)\n",
    "\n",
    "    fnret = re.search(r'.*?paper.([0-9]+.*)', url)\n",
    "    if not fnret:\n",
    "        raise RuntimeError, \"error parsing paper url '%s'\" % (url)\n",
    "\n",
    "    basename = fnret.group(1)\n",
    "    basename = strip_slashes(basename)\n",
    "    pdf_file = basename + \".pdf\"\n",
    "    bib_file = basename + \".bib\"\n",
    "\n",
    "    soup = BeautifulSoup(paperpage.text)\n",
    "    pdfurls = soup.findAll(u\"meta\", attrs={u\"name\": u\"citation_pdf_url\"})\n",
    "    lencheck(pdfurls)\n",
    "    pdfurl = pdfurls[0][u\"content\"]\n",
    "    biburls = soup.findAll(u\"a\", attrs={u\"href\":\n",
    "                                        re.compile(r'bibtex$')})\n",
    "    lencheck(biburls)\n",
    "    biburl = BASEURL + biburls[0][u\"href\"]\n",
    "    print \" downloading %s ...\" % (basename),\n",
    "    sys.stdout.flush()\n",
    "    if not os.path.exists(pdf_file):\n",
    "        urllib.urlretrieve(pdfurl, pdf_file)\n",
    "    print \"pdf\",\n",
    "    sys.stdout.flush()\n",
    "    if not os.path.exists(bib_file):\n",
    "        urllib.urlretrieve(biburl, bib_file)\n",
    "    print \"bib.\"\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def download_single_paper(year, paper_number):\n",
    "    yearpage = get_year_page(year)\n",
    "    for url, pn in get_all_papers_on_yearpage(yearpage):\n",
    "        try:\n",
    "            pn = int(pn)\n",
    "        except ValueError:\n",
    "            raise RuntimeError, \"error parsing yearpage\"\n",
    "        if pn == paper_number:\n",
    "            download_pdf_from_paperpage(url)\n",
    "            return\n",
    "    raise RuntimeError, \"paper %i not found\" % (paper_number)\n",
    "\n",
    "\n",
    "def download_all_papers(year):\n",
    "    yearpage = get_year_page(year)\n",
    "    for url, paper_number in get_all_papers_on_yearpage(yearpage):\n",
    "        if(paper_number == 6767):\n",
    "            download_pdf_from_paperpage(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from 2017 proceedings all papers\n"
     ]
    }
   ],
   "source": [
    "year = 2017\n",
    "paper_number = None\n",
    "\n",
    "print \"Downloading from %i proceedings\" % (year),\n",
    "if paper_number is not None:\n",
    "    print \"paper no. %i\" % (paper_number)\n",
    "    download_single_paper(year, paper_number)\n",
    "else:\n",
    "    print \"all papers\"\n",
    "    download_all_papers(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
